{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "### Due: Fri Dec 18th @ 11:59pm ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will covering NLP, Topic Modeling and Recommendation Engines\n",
    "\n",
    "We will generate recommendations on products from a department store based on product descriptions.\n",
    "We'll first transform the data into topics using Latent Dirichlet Allocation, and then generate recommendations based on this new representation using Content-Based Filtering.\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- **Follow the comments below and fill in the blanks (____)**\n",
    "- **Please use default arguments whenever arguments aren't specified.**\n",
    "- **Please 'Restart and Run All' prior to submission.**\n",
    "- **When submitting to Gradescope, please mark on which page each question is answered.**\n",
    "\n",
    "\n",
    "Out of 28 points total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and Recommendation Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a recommendation engine for products from a department store.  \n",
    "The recommendations will be based on the similarity of product descriptions.  \n",
    "We'll use Content-Filtering to query a product and get back a list of products that are similar.  \n",
    "Instead of using the descriptions directly, we will first do some topic modeling using LDA to transform the descriptions into a topic space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform product descriptions into topics and print sample terms from topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. (2pts) Load the Data\n",
    "\n",
    "# The dataset we'll be working with is a set of product descriptions from the department store JCPenney.\n",
    "\n",
    "# Load product information from ../data/jcpenney-products_subset.csv.zip\n",
    "# This is a compressed version of a csv file.\n",
    "# Use pandas read_csv function with the default parameters.\n",
    "# read_csv has an argument 'compression' with default value 'infer' that will handle unzipping the data.\n",
    "# There is no need to unzip the data prior to using read_csv.\n",
    "# Store the resulting dataframe as df_products.\n",
    "____\n",
    "\n",
    "# print a summary of df_products using .info. There should be 5000 records and 6 columns\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. (2pts) Print an Example\n",
    "\n",
    "# The two columns of the dataframe we're interested in are:\n",
    "#   'name_title' which is the name of the product stored as a string\n",
    "#   'description' which is a description of the product stored as a string\n",
    "#\n",
    "# We'll print out the product in the first row as an example\n",
    "# If we try to print both columns at the same time, pandas will truncate the strings\n",
    "#   so we'll print them seperately\n",
    "\n",
    "# print the column 'name_title' in row 0 of df_products\n",
    "____\n",
    "\n",
    "print('---') # to visually separate the two strings\n",
    "\n",
    "# print the column 'desciption' in row 0 of df_products\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. (4pts) Transform Descriptions using TfIdf\n",
    "\n",
    "# In order to pass our product descriptions to the LDA model, we first need to vectorize \n",
    "#   from strings to fixed-length feature vectors.\n",
    "# To do this we will transform our documents using Tf-Idf,\n",
    "\n",
    "# Import TfidfVectorizer from sklearn.feature_extraction.text\n",
    "____\n",
    "\n",
    "#  Instantiate a TfidfVectorizer with\n",
    "#   ngram_range=(1,2), use both unigrams and bigrams\n",
    "#   min_df=20,         excluding terms which appear in less than 20 documents\n",
    "#   max_df=.7          excluding terms which appear in more than 70% of documents   \n",
    "#   all other arguments as their default\n",
    "\n",
    "# Store as tfidf\n",
    "____\n",
    "\n",
    "# fit_transform tfidf on the 'description' column of the df_products dataframe \n",
    "# Store the transformed dataset as X_tfidf\n",
    "X_tfidf = tfidf.fit_transform(df_products.description)\n",
    "\n",
    "# Assert that the shape of X_tfidf is 5000 rows by 2732 columns\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. (4pts) Format Bigrams and Print Sample of Extracted Vocabulary \n",
    "\n",
    "# The extracted vocabulary can be retrieved from tfidf as a list using .get_feature_names()\n",
    "# Store the extracted vocabulary list in the variable vocab\n",
    "____\n",
    "\n",
    "# Sklearn joins bigrams with a space character.\n",
    "# To make output easier to read, replace all spaces in the vocab list with underscores.\n",
    "# To do this we can use the string .replace() method.\n",
    "# For example, if x is a string, x.replace(' ','_') will replace all ' ' with '_' in x.\n",
    "# A list comprehension would be useful here, but use any method you like\n",
    "#    to iterate through each item in vocab, replacing spaces with underscores.\n",
    "# Store the result back into vocab\n",
    "____\n",
    "\n",
    "# Print the last 5 terms in the vocabulary (should end with 'zippered')\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. (4pts) Perform Topic Modeling with LDA\n",
    "\n",
    "# Now that we have our vectorized data, we can use Latent Direchlet Allocation to learn \n",
    "#   the per-document topic distributions and per-topic term distributions.\n",
    "# Though there are likely more, we'll model our dataset using 20 topics to keep things small.\n",
    "\n",
    "# Import LatentDirichletAllocation from sklearn.decomposition\n",
    "____\n",
    "\n",
    "# Instantiate a LatentDirichletAllocation model with\n",
    "#    n_components=20    fit 20 topics\n",
    "#    n_jobs=-1,         use all cores\n",
    "#    random_state=123   for reproducability\n",
    "# Store the model as lda\n",
    "____\n",
    "\n",
    "# Run fit_transform() on lda using X_tfidf.\n",
    "# Store the output (the per-document topic distributions) as X_lda\n",
    "____\n",
    "\n",
    "# Assert that the shape of X_lda is 5000 rows by 20 columns\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. (5pts) Print Top Topic Terms\n",
    "\n",
    "# To get a sense of what each topic is composed of, we can print the most likely terms for each topic.\n",
    "# For each topic print 'Topic {topic_idx:2d} : ' followed by \n",
    "#   the top 5 most likely terms in that topic given the per-topic term distribution\n",
    "# Example:\n",
    "#    Topic  0 : wicking moisture moisture_wicking dri fabric\n",
    "\n",
    "# We'll use the vocab created above, but first convert from a list to np.array to make indexing easier\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "# This function returns a list of the top n terms in vocab given the weights in term_weights\n",
    "def get_top_terms(term_weights, top_n=5):\n",
    "    # np.argsort() returns the indices of an np.array sorted by their value, in ascending order\n",
    "    # [::-1] reverses the order of an np.array (descending order)\n",
    "    # list() converts from an np.array() back to a list\n",
    "    return list(vocab[np.argsort(term_weights)[::-1]][:top_n])\n",
    "\n",
    "# The per-topic term distributions (term_weights) are stored in lda.components_\n",
    "# For each array of term_weights in lda.components_ \n",
    "#    use get_top_terms() to print the top 5 terms per topic.\n",
    "# Example:\n",
    "#    Topic  0 : wicking moisture moisture_wicking dri fabric\n",
    "# Hints:\n",
    "#   use enumerate to get a topic_idx and the term_weights from lda.components_\n",
    "#   prefix each line with the string produced by f'Topic {topic_idx:2d} : '\n",
    "#   use ' '.join() to join the list of terms returned by get_top_terms()\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate recommendations using topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. (3pts) Generate Similarity Matrix\n",
    "\n",
    "# We'll use Content-Based Filtering to make recommendations based on a query product.\n",
    "# Each product will be represented by its LDA topic weights learned above.\n",
    "# We'd like to recommend products similar in LDA space.\n",
    "# We'll use euclidan distance as a measure of similarity.\n",
    "\n",
    "# Import euclidean_distances from sklearn.metrics.pairwise \n",
    "____\n",
    "\n",
    "# Use euclidean_distances to generate similarity scores on our X_lda data\n",
    "# Recall that when using distance as similarity, smaller is better.\n",
    "# Store in a variable named distances.\n",
    "# NOTE: we only need to pass X_lda in once,\n",
    "#   the function will calculate pairwise distances for all rows in that matrix\n",
    "____\n",
    "\n",
    "# Assert that the shape of the similarities matrix is 5000 rows by 5000 columns\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.(4pts) Generate Recommendations\n",
    "\n",
    "# Let's test our proposed recommendation engine using the product at row 0 in df_products as the query.\n",
    "#   The name of this product is \"Alfred Dunner® Essential Pull On Capri Pant\"\n",
    "\n",
    "# Print the names for the top 10 most similar products to this query.\n",
    "# Suggested way to do this is:\n",
    "#   get the euclidean distances from row 0 of the distances matrix\n",
    "#   get the indices of this array sorted by value using np.argsort()\n",
    "#   get the first 10 indexes from this reversed array\n",
    "#   use those indices to index into df_products.name_title and print the result\n",
    "\n",
    "# NOTE: The first product should be:\n",
    "#   'Alfred Dunner® Essential Pull On Capri Pant', (the original query product)\n",
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f20",
   "language": "python",
   "name": "eods-f20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
